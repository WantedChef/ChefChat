active_model = "big-pickle"
vim_keybindings = false
disable_welcome_banner_animation = false
displayed_workdir = ""
auto_compact_threshold = 200000
context_warnings = false
textual_theme = "textual-dark"
instructions = ""
system_prompt_id = "cli"
include_commit_signature = true
include_model_info = true
include_project_context = true
include_prompt_detail = true
enable_update_checks = true
api_timeout = 720.0
tool_paths = []
mcp_servers = []
enabled_tools = []
disabled_tools = []

[[providers]]
name = "mistral"
api_base = "https://api.mistral.ai/v1"
api_key_env_var = "MISTRAL_API_KEY"
api_style = "openai"
backend = "mistral"

[[providers]]
name = "openai"
api_base = "https://api.openai.com/v1"
api_key_env_var = "OPENAI_API_KEY"
api_style = "openai"
backend = "generic"

[[providers]]
name = "groq"
api_base = "https://api.groq.com/openai/v1"
api_key_env_var = "GROQ_API_KEY"
api_style = "openai"
backend = "generic"

[[providers]]
name = "openrouter"
api_base = "https://openrouter.ai/api/v1"
api_key_env_var = "OPENROUTER_API_KEY"
api_style = "openai"
backend = "generic"

[[providers]]
name = "opencode"
api_base = "https://opencode.ai/zen/v1"
api_key_env_var = "ZEN_API_KEY"
api_style = "openai"
backend = "generic"

[[models]]
alias = "mistral-small"
name = "mistral-small-latest"
provider = "mistral"
temperature = 0.2
input_price = 0.2
output_price = 0.6
max_tokens = 32000
features = [
    "speed",
    "coding",
    "tool_use",
    "cost_effective",
]

[[models]]
alias = "mistral-large"
name = "mistral-large-latest"
provider = "mistral"
temperature = 0.2
input_price = 2.0
output_price = 6.0
max_tokens = 128000
features = [
    "reasoning",
    "coding",
    "tool_use",
    "large_context",
]

[[models]]
alias = "devstral-2512"
name = "codestral-latest"
provider = "mistral"
temperature = 0.2
input_price = 0.4
output_price = 2.0

[[models]]
alias = "devstral-2"
name = "mistral-small-latest"
provider = "mistral"
temperature = 0.2
input_price = 0.2
output_price = 0.6

[[models]]
alias = "mistral-vibe-cli"
name = "codestral-2501"
provider = "mistral"
temperature = 0.2
input_price = 0.4
output_price = 2.0

[[models]]
alias = "gpt35"
name = "gpt-3.5-turbo"
provider = "openai"
temperature = 0.2
input_price = 0.5
output_price = 1.5

[[models]]
alias = "groq-8b"
name = "llama-3.1-8b-instant"
provider = "groq"
temperature = 0.2
input_price = 0.05
output_price = 0.08
max_tokens = 131072
features = [
    "speed",
    "low_cost",
]

[models.rate_limits]
tpm = 250000
rpm = 1000

[[models]]
alias = "groq-70b"
name = "llama-3.3-70b-versatile"
provider = "groq"
temperature = 0.2
input_price = 0.59
output_price = 0.79
max_tokens = 32768
features = [
    "versatile",
    "balanced",
]

[models.rate_limits]
tpm = 300000
rpm = 1000

[[models]]
alias = "llama-scout"
name = "meta-llama/llama-4-scout-17b-16e-instruct"
provider = "groq"
temperature = 0.2
input_price = 0.11
output_price = 0.34
max_tokens = 8192
multimodal = true
max_file_size = 20
features = [
    "multimodal",
    "vision",
    "tool_use",
    "fast",
]

[models.rate_limits]
tpm = 300000
rpm = 1000

[[models]]
alias = "llama-maverick"
name = "meta-llama/llama-4-maverick-17b-128e-instruct"
provider = "groq"
temperature = 0.2
input_price = 0.2
output_price = 0.6
max_tokens = 8192
multimodal = true
max_file_size = 20
features = [
    "multimodal",
    "vision",
    "tool_use",
    "premium",
]

[models.rate_limits]
tpm = 300000
rpm = 1000

[[models]]
alias = "kimi-k2"
name = "moonshotai/kimi-k2-instruct-0905"
provider = "groq"
temperature = 0.3
input_price = 1.0
output_price = 3.0
max_tokens = 16384
features = [
    "reasoning",
    "large_context",
    "coding",
]

[models.rate_limits]
tpm = 250000
rpm = 1000

[[models]]
alias = "gpt-oss-120b"
name = "openai/gpt-oss-120b"
provider = "groq"
temperature = 0.2
input_price = 0.15
output_price = 0.6
max_tokens = 65536
features = [
    "reasoning",
    "browser_tools",
    "balanced",
]

[models.rate_limits]
tpm = 250000
rpm = 1000

[[models]]
alias = "gpt-oss-20b"
name = "openai/gpt-oss-20b"
provider = "groq"
temperature = 0.2
input_price = 0.075
output_price = 0.3
max_tokens = 65536
features = [
    "speed",
    "browser_tools",
    "cost_effective",
]

[models.rate_limits]
tpm = 250000
rpm = 1000

[[models]]
alias = "qwen-32b"
name = "qwen/qwen3-32b"
provider = "groq"
temperature = 0.2
input_price = 0.29
output_price = 0.59
max_tokens = 40960
features = [
    "multilingual",
    "balanced",
]

[models.rate_limits]
tpm = 300000
rpm = 1000

[[models]]
alias = "or-claude-sonnet"
name = "anthropic/claude-3.5-sonnet"
provider = "openrouter"
temperature = 0.2
input_price = 3.0
output_price = 15.0
max_tokens = 8192
multimodal = true
features = [
    "reasoning",
    "coding",
    "vision",
    "tool_use",
]

[[models]]
alias = "or-claude-haiku"
name = "anthropic/claude-3.5-haiku"
provider = "openrouter"
temperature = 0.2
input_price = 0.8
output_price = 4.0
max_tokens = 8192
features = [
    "speed",
    "coding",
    "tool_use",
]

[[models]]
alias = "or-gemini-flash"
name = "google/gemini-2.0-flash-exp:free"
provider = "openrouter"
temperature = 0.2
input_price = 0.0
output_price = 0.0
max_tokens = 8192
multimodal = true
features = [
    "speed",
    "vision",
    "free",
]

[[models]]
alias = "or-deepseek"
name = "deepseek/deepseek-chat"
provider = "openrouter"
temperature = 0.2
input_price = 0.14
output_price = 0.28
max_tokens = 8192
features = [
    "coding",
    "reasoning",
    "cost_effective",
]

[[models]]
alias = "or-deepseek-coder"
name = "deepseek/deepseek-coder"
provider = "openrouter"
temperature = 0.2
input_price = 0.14
output_price = 0.28
max_tokens = 16384
features = [
    "coding",
    "tool_use",
    "cost_effective",
]

[[models]]
alias = "or-qwen-coder"
name = "qwen/qwen-2.5-coder-32b-instruct"
provider = "openrouter"
temperature = 0.2
input_price = 0.07
output_price = 0.16
max_tokens = 32768
features = [
    "coding",
    "large_context",
    "cost_effective",
]

[[models]]
alias = "or-llama-70b"
name = "meta-llama/llama-3.3-70b-instruct"
provider = "openrouter"
temperature = 0.2
input_price = 0.39
output_price = 0.39
max_tokens = 8192
features = [
    "versatile",
    "open_source",
    "tool_use",
]

[[models]]
alias = "or-mistral-large"
name = "mistralai/mistral-large-2411"
provider = "openrouter"
temperature = 0.2
input_price = 2.0
output_price = 6.0
max_tokens = 32768
features = [
    "reasoning",
    "coding",
    "multilingual",
]

[[models]]
alias = "zen-1"
name = "gpt-5"
provider = "opencode"
temperature = 0.2
input_price = 0.0
output_price = 0.0
max_tokens = 32768
features = [
    "coding",
    "reasoning",
    "tool_use",
]

[[models]]
alias = "zen-mini"
name = "gpt-5-nano"
provider = "opencode"
temperature = 0.2
input_price = 0.0
output_price = 0.0
max_tokens = 16000
features = [
    "speed",
    "coding",
    "cost_effective",
]

[[models]]
alias = "grok-fast"
name = "grok-code"
provider = "opencode"
temperature = 0.2
input_price = 0.0
output_price = 0.0
max_tokens = 16000
features = [
    "coding",
    "speed",
    "free",
]

[[models]]
alias = "alpha-gd4"
name = "alpha-gd4"
provider = "opencode"
temperature = 0.0
input_price = 0.0
output_price = 0.0
max_tokens = 32768
features = [
    "experimental",
    "free",
]

[[models]]
alias = "big-pickle"
name = "big-pickle"
provider = "opencode"
temperature = 0.2
input_price = 0.0
output_price = 0.0
max_tokens = 32768
features = [
    "experimental",
    "free",
]

[project_context]
max_chars = 40000
default_commit_count = 5
max_doc_bytes = 32768
truncation_buffer = 1000
max_depth = 3
max_files = 1000
max_dirs_per_level = 20
timeout_seconds = 2.0

[session_logging]
save_dir = "/home/chef/.vibe/logs/session"
session_prefix = "session"
enabled = true

[tools.grep]
permission = "always"
allowlist = []
denylist = []
max_output_bytes = 64000
default_max_matches = 100
default_timeout = 60
exclude_patterns = [
    ".venv/",
    "venv/",
    ".env/",
    "env/",
    "node_modules/",
    ".git/",
    "__pycache/",
    ".pytest_cache/",
    ".mypy_cache/",
    ".tox/",
    ".nox/",
    ".coverage/",
    "htmlcov/",
    "dist/",
    "build/",
    ".idea/",
    ".vscode/",
    "*.egg-info",
    "*.pyc",
    "*.pyo",
    "*.pyd",
    ".DS_Store",
    "Thumbs.db",
]
codeignore_file = ".vibeignore"

[tools.todo]
permission = "always"
allowlist = []
denylist = []
max_todos = 100

[tools.search_replace]
permission = "ask"
allowlist = []
denylist = []
max_content_size = 100000
create_backup = false
fuzzy_threshold = 0.9

[tools.write_file]
permission = "ask"
allowlist = []
denylist = []
max_write_bytes = 64000
create_parent_dirs = true

[tools.read_file]
permission = "always"
allowlist = []
denylist = []
max_read_bytes = 64000
max_state_history = 10

[tools.bash]
permission = "ask"
allowlist = [
    "echo",
    "find",
    "git diff",
    "git log",
    "git status",
    "tree",
    "whoami",
    "cat",
    "file",
    "head",
    "ls",
    "pwd",
    "stat",
    "tail",
    "uname",
    "wc",
    "which",
    "gemini",
    "curl",
    "wget",
    "grep",
    "sed",
    "awk",
    "sort",
    "uniq",
    "cut",
    "tr",
    "xargs",
    "date",
    "hostname",
    "df",
    "du",
    "free",
    "top",
    "ps",
    "netstat",
    "ss",
    "ip",
    "ping",
    "dig",
    "pdb",
    "passwd",
    "nano",
    "vim",
    "vi",
    "emacs",
    "bash -i",
    "sh -i",
    "zsh -i",
    "fish -i",
    "dash -i",
    "screen",
    "tmux",
    "nslookup",
    "kilocode",
    "opencode",
    "claude",
    "aider",
    "cody",
]
denylist = []
max_output_bytes = 16000
default_timeout = 30
denylist_standalone = [
    "python",
    "python3",
    "ipython",
    "bash",
    "sh",
    "nohup",
    "vi",
    "vim",
    "emacs",
    "nano",
    "su",
]
